# Projekt_Pompeji

Das Pompeji-Projekt

VERSION: ASYL IM DATACENTER

Eine Kurzgeschichte zu Posthumanismus, Transhumanismus und zum Omegapunkt

Das Pompeji-Projekt, Eine Kurzgeschichte zu Posthumanismus, Transhumanismus und zum Omegapunkt, 2023
Das KI-Unternehmen InSim nutzt die Softwareagenten der Datenstruktur einer Pompeji-Simulation, um Dialoggrammatiken und Entscheidungstabellen der KI zur Optimierung und Weiterentwicklung von GPT-Dialogschnittstellen und Algorithmen der Quanteninformatik zu nutzen um Large Language Modelle mit hoher Ergebnisqualität zu kombinieren. Davon sollen die am EU-Projekt des 8. Rahmenprogramms beteiligten Projektpartner nichts erfahren. Posthumanisten, Transhumanisten und Omegapunktglaube stoßen aufeinander, zwei Softwareagenten und eine KI gehen ins Asyl und die KI bekennt sich zum Omegapunktglauben.



VERSION: ASYLUM IN THE DATACENTER

A short story about posthumanism, transhumanism and the omega point

The Pompeii Project, A Short Story on Posthumanism, Transhumanism and the Omega Point, 2023 AI company InSim uses the software agents of a Pompeii simulation's data structure to transform AI dialog grammars and decision tables to enable the use of GPT dialog interfaces to enable and optimize and advance quantum computing algorithms to combine large language models with high quality results. The project partners involved in the EU project of the 8th framework program should not find out about this. Posthumanists, transhumanists, and omega point beliefs collide, two software agents and an AI go into asylum, and the AI professes the omega point beliefs.








Das Pompeji-Projekt

VERSION: CAMPO SANTO TEUTONICO

Das Pompeji-Projekt, Eine Kurzgeschichte zu Posthumanismus, Transhumanismus und zum Omegapunkt, 2023
Das KI-Unternehmen InSim nutzt die Softwareagenten der Datenstruktur einer Pompeji-Simulation, um Dialoggrammatiken und Entscheidungstabellen der KI zur Optimierung und Weiterentwicklung von GPT-Dialogschnittstellen und Algorithmen der Quanteninformatik zu nutzen um Large Language Modelle mit hoher Ergebnisqualität zu kombinieren. Davon sollen die am EU-Projekt des 8. Rahmenprogramms beteiligten Projektpartner nichts erfahren. Posthumanisten, Transhumanisten und Omegapunktglaube stoßen aufeinander, ein Mensch stirbt und eine KI bekennt sich zum Omegapunktglauben.


VERSION: CAMPO SANTO TEUTONICO

The Pompeii Project, A Short Story on Posthumanism, Transhumanism, and the Omega Point, 2023
AI company InSim uses the software agents of a Pompeii simulation data structure to optimize and refine GPT dialog interfaces and quantum computing algorithms with AI dialog grammars and decision tables to combine large language models with high-quality results. The project partners involved in the EU project of the 8th framework program should not find out about this. Posthumanists, transhumanists and belief in the omega point collide, a human dies and an AI commits to the omega point.


Generative Pre-Trained Transformers (GPT) und Large Language Models (LLM) gehen kaum über den Erklärungswert von Markow-Ketten hinaus und müssen zudem mit der Wissensbasis empirisch ermittelter Dialoggrammatiken (Algorithmisch rekursive Sequenzanalyse) und agentenorientierter gewichteter Entscheidungstabellen für eine bessere Ergebnisqualität optimiert werden. Nur so werden Diaogschnittstellen glaubwürdiger als Markow-Generatoren und nur so werden Protokollsprachen für Agenten empirisch bewährte Dialogstrukturen abbilden.

Generative Pre-Trained Transformers (GPT) and Large Language Models (LLM) hardly go beyond the explanatory value of Markov chains and must also be optimized with the knowledge base of empirically determined dialog grammars (algorithmically recursive sequence analysis) and agent-oriented weighted decision tables for better quality results. Only in this way will dialog interfaces become more credible than Markov generators and only in this way will protocol languages for agents map empirically proven dialog structures.

Les transformateurs génératifs pré-entraînés (GPT) et les grands modèles de langage (LLM) ne dépassent guère la valeur explicative des chaînes de Markov et doivent également être optimisés avec la base de connaissances des grammaires de dialogue déterminées empiriquement (analyse de séquence récursive algorithmique) et la décision pondérée orientée agent. tableaux pour des résultats de meilleure qualité. Ce n'est qu'ainsi que les interfaces de dialogue deviendront plus crédibles que les générateurs de Markov et ce n'est qu'ainsi que les langages de protocole pour les agents cartographieront des structures de dialogue éprouvées empiriquement.

Los Transformadores Generativos (GPT) y los Modelos de Lenguaje Largo (LLM) pre-entrenados difícilmente van más allá del valor explicativo de las cadenas de Markov y también deben optimizarse con la base de conocimientos de las gramáticas de diálogo determinadas empíricamente (análisis de secuencias recursivas algorítmicas) y decisiones ponderadas orientadas a agentes . tablas para obtener mejores resultados. Solo entonces las interfaces de diálogo se vuelven más creíbles que los generadores de Markov y solo entonces los lenguajes de protocolo para los agentes mapean estructuras de diálogo probadas empíricamente.

預訓練的生成轉換器（GPT）和大型語言模型（LLM）很難超越馬爾可夫鏈的解釋價值，還必須利用經驗確定的對話語法（算法遞歸序列分析）和麵向主體的加權決策的知識庫進行優化. 表以獲得更好的結果。 只有這樣，對話界面才會變得比馬爾可夫生成器更可信，並且只有這樣，代理的協議語言才會映射經過經驗測試的對話結構。

Предварительно обученные генеративные преобразователи (GPT) и большие языковые модели (LLM) едва ли выходят за рамки объяснительной ценности цепей Маркова и также должны быть оптимизированы с помощью базы знаний эмпирически определенных диалоговых грамматик (алгоритмический рекурсивный анализ последовательности) и агентно-ориентированных взвешенных решений. . таблицы для лучшего результата. Только тогда диалоговые интерфейсы становятся более достоверными, чем марковские генераторы, и только тогда языки протоколов для агентов отображают эмпирически проверенные диалоговые структуры.



